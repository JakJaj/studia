{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21.11.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importujemy potrzebne nam biblioteki\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Budowanie modelu sieci konwolucyjnej\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1 (Conv2D)              (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " maxpool1 (MaxPooling2D)     (None, 13, 13, 32)        0         \n",
      "                                                                 \n",
      " conv2 (Conv2D)              (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " maxpool2 (MaxPooling2D)     (None, 5, 5, 64)          0         \n",
      "                                                                 \n",
      " conv3 (Conv2D)              (None, 3, 3, 64)          36928     \n",

      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55744 (217.75 KB)\n",
      "Trainable params: 55744 (217.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',name='conv1',input_shape=(28,28,1), padding = 'valid'))\n",
    "model.add(layers.MaxPooling2D((2, 2), name='maxpool1'))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', name='conv2'))\n",
    "model.add(layers.MaxPooling2D((2, 2), name='maxpool2'))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', name='conv3'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.add(layers.Flatten(name='Flatten'))\n",
    "model.add(layers.Dense(64, activation='relu', name='Dense1'))\n",
    "model.add(layers.Dense(23, activation= 'softmax', name='Dense2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dane ze zbioru Sign Language MNIST\n",
    "___\n",
    "Zbior zawiera obrazy 28x28 w skali szarosci\n",
    "\n",
    "Kazdy obraz przedstawia dlog prezentujaca litere alfabetu migowego\n",
    "\n",
    "Warto nadmienic ze w zbiorze tym nie znajdziemy litery J poniewaz do wykonania tej litery wymagany jest dodatkowy ruch dlonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('sign_mnist_train.csv')\n",
    "data_test = data=pd.read_csv('sign_mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7172, 785)\n",
      "(7172, 785)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponizej widzimy ze w liscie labeli nie znajduje sie nr 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(data.label.unique())) #ponizej widac brak klasy nr 9\n",
    "print(sorted(data_test.label.unique())) #ponizej widac brak klasy nr 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby pozbyc sie problemu brakujacej klasy kazdy label wiekszy od 9 zmniejszamy o 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_class_labels(label):\n",
    "   \n",
    "    if label >= 10:\n",
    "        label -= 1\n",
    "    \n",
    "    return label\n",
    "\n",
    "# Adjusting the class labels (training data)\n",
    "data[\"label\"] = data[\"label\"].apply(adjust_class_labels)\n",
    "data_test[\"label\"] = data_test[\"label\"].apply(adjust_class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(data.label.unique()))\n",
    "print(sorted(data_test.label.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rysujemy dwa pierwsze obrazy zeby upewnic sie ze dane zostaly wczytane prawidolowo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "images=data.drop('label',axis=1).values\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "for i in range(2): \n",
    "    img=images[i].reshape(28,28)\n",
    "    fig = plt.subplot(4,4,i+1)\n",
    "    plt.xlabel(data['label'][i])\n",
    "    plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z danych testowych pozbywamy sie etykiet i etykiety te przypisujemy do oddzielnego zbioru przeznaczonego na etykiety wlasnie\n",
    "\n",
    "Nastepnie ustalamy odpowiedni wymiar danych i przeskalowujemy nasze dane na wartosci z przedzialu [0,1]\n",
    "\n",
    "Na koniec zakodywujemy etykiety za pomocÄ… kategorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7172, 28, 28, 1)\n",
      "(7172, 23)\n",
      "(7172, 28, 28, 1)\n",
      "(7172, 23)\n"
     ]
    }
   ],
   "source": [
    "x_train = data.drop('label',axis=1)\n",
    "x_test = data_test.drop('label', axis=1)\n",
    "\n",
    "y_train = data.label\n",
    "y_test = data_test.label\n",
    "\n",
    "x_train = x_train.values.reshape(-1,28,28,1)\n",
    "x_train = x_train/255\n",
    "\n",
    "x_test = x_test.values.reshape(-1,28,28,1)\n",
    "x_test = x_test/255\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train) \n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jesli istnieje folder z logami naszego modelu to pozbywamy sie go zeby dane nowego modelu mozna bylo pozniej uzyc w tensorboardzie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf ./logs/fit/first/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzymy callback za pomoca ktorego bedziemy mogli potem otrzymac informacje o procesie uczenia modelu w tensorboardzie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/first/\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kompilujemy i fitujemy model z wczesniej zdefiniowanym callbackiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 3.1019 - accuracy: 0.0821 - val_loss: 3.0577 - val_accuracy: 0.1004\n",
      "Epoch 2/15\n",
      "157/157 [==============================] - 1s 10ms/step - loss: 3.0262 - accuracy: 0.1361 - val_loss: 2.9608 - val_accuracy: 0.1808\n",
      "Epoch 3/15\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 2.8865 - accuracy: 0.1849 - val_loss: 2.7643 - val_accuracy: 0.2063\n",
      "Epoch 4/15\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 2.6340 - accuracy: 0.2534 - val_loss: 2.4631 - val_accuracy: 0.2844\n",
      "Epoch 5/15\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 2.3489 - accuracy: 0.3175 - val_loss: 2.2044 - val_accuracy: 0.3666\n",
      "Epoch 6/15\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 2.1005 - accuracy: 0.3855 - val_loss: 1.9670 - val_accuracy: 0.4173\n",
      "Epoch 7/15\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 1.8903 - accuracy: 0.4502 - val_loss: 1.7880 - val_accuracy: 0.4670\n",
      "Epoch 8/15\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 1.7064 - accuracy: 0.5062 - val_loss: 1.6071 - val_accuracy: 0.5218\n",
      "Epoch 9/15\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 1.5394 - accuracy: 0.5584 - val_loss: 1.4843 - val_accuracy: 0.5474\n",
      "Epoch 10/15\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 1.3887 - accuracy: 0.5984 - val_loss: 1.3398 - val_accuracy: 0.6004\n",
      "Epoch 11/15\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 1.2459 - accuracy: 0.6414 - val_loss: 1.1901 - val_accuracy: 0.6557\n",
      "Epoch 12/15\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 1.1249 - accuracy: 0.6823 - val_loss: 1.0821 - val_accuracy: 0.6961\n",
      "Epoch 13/15\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 1.0150 - accuracy: 0.7157 - val_loss: 0.9741 - val_accuracy: 0.7240\n",
      "Epoch 14/15\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.9170 - accuracy: 0.7508 - val_loss: 0.8844 - val_accuracy: 0.7393\n",
      "Epoch 15/15\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.8289 - accuracy: 0.7789 - val_loss: 0.7832 - val_accuracy: 0.7788\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.legacy.RMSprop(learning_rate=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history=model.fit(x_train, y_train, epochs=15 ,validation_split=0.3, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzamy jak model da sobie rade na danych testowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 1s 3ms/step - loss: 0.7783 - accuracy: 0.7814\n",
      "Dokladnosc modelu na danych testowych to: 0.7813720107078552 ze strata rowna 0.7782968878746033\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test,y_test)\n",
    "print(f\"Dokladnosc modelu na danych testowych to: {test_accuracy} ze strata rowna {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby wykorzystac mozliwosci ktore gwarantuje nam TensorBoard nalezy uzyc nastepujacej komendy w konsoli\n",
    "\n",
    "```sh\n",
    "tensorboard --logdir logs/fit/first/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit/first/ --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informacje z tensorboard\n",
    "* Model uczy sie\n",
    "* Model powinien uczyc sie wieksza liczbe epok poniewaz:\n",
    "    * Dokladnosc rosnie\n",
    "    * Strata maleje\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28.11.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuner\n",
    "\n",
    "W ponizszej implementacji funkcji build_model do tunera modelu konwolucyjnej sieci neuronowej tunujemy nastepujace hiperparametry:\n",
    "* Liczbe warst Conv2D i co za tym idzie rowniez liczbe warstw Pooling\n",
    "* W warstwie Conv2D tunujemy:\n",
    "    * Liczbe filtrow z przedzialu [32,256]\n",
    "    * Wielkosc kernela przechodzacego przez obraz z przedzialu [3,5]\n",
    "* Rodzaj warstwy pooling wybieramy pomiedzy:\n",
    "    * MaxPooling - w tym przypadku filtr bierze wartosc maksymalna w danym kernelu\n",
    "    * AvgPooling - w tym przypadku filtr bierze wartosc srednia w danym kernelu\n",
    "* Liczbe warstw lub wartstwy gestej wystepujacych po warstwie splaszczajacej w tym przypadku wybieramy:\n",
    "    * Liczbe warstw gestych\n",
    "    * Liczbe neuronow w kazdej warstwie\n",
    "* W warstwie Dropout tunujemy dropout rate, aby ograniczyc przetrenowanie\n",
    "* Wspolczynnik uczenia\n",
    "* Optimizer w ktorym wybieramy pomiedzy:\n",
    "    * Stochastycznym spadkiem wzdluz gradientu, SGD\n",
    "    * Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    for i in range(hp.Int('num_conv_layers', min_value=1, max_value=4)): #tunujemy liczbe warstw Conv2D\n",
    "        model.add(layers.Conv2D(\n",
    "            filters=hp.Int(f'{i}_conv_filters', min_value=32, max_value=256, step=32), #tunowanie liczby filtrow\n",
    "            kernel_size=hp.Choice(f'{i}_conv_kernel_size', values=[3, 5]), #tunowanie wielkosci kernela\n",
    "            activation='relu',\n",
    "            input_shape=(28, 28, 1),\n",
    "            name=f'{i}_conv_layer' \n",
    "        ))\n",
    "        \n",
    "        pooling = hp.Choice(f'{i}_pooling', values=['max','avg']) #tunowanie wyboru warstwy pooling \n",
    "        \n",
    "        if pooling == 'max':\n",
    "            model.add(layers.MaxPooling2D(pool_size=(2, 2),name=f'{i}_max_pooling')) \n",
    "        else:\n",
    "            model.add(layers.AveragePooling2D(pool_size=(2, 2), name=f'{i}_avg_pooling'))\n",
    "\n",
    "    model.add(layers.Flatten(name='flatten')) #splaszczanie\n",
    "\n",
    "    for j in range(hp.Int('num_dense_layers', min_value=1, max_value=3)): #tunowanie liczby warstw gestych\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f'{j}_n_neurons', min_value=32, max_value=512, step=32), #tunowanie liczby neuronow w warstwie gestej\n",
    "            activation='relu',\n",
    "            name=f'{j}_dence_layer'\n",
    "            ))\n",
    "        \n",
    "        model.add(layers.Dropout(rate=hp.Float(f'{j}_dropout', min_value=0.0, max_value=0.5, step=0.1))) #tunowanie dropout rate\n",
    "\n",
    "    model.add(layers.Dense(23, activation='softmax',name='output_layer')) \n",
    "\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\") #tunowanie learning rate\n",
    "    optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"]) #tunowanie optimizera\n",
    "    \n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "        \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',  \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiujemy nastepujace callbacki:\n",
    "* tensorboard_callback - aby mozliwe bylo uzywanie tensorboard_callback\n",
    "* early_stopping_cb - aby w momencie kiedy model przestaje sie uczyc. Uczenie sie zatrzymywalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=8,restore_best_weights=True)\n",
    "log_dir = \"logs/fit/tuner/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiujemy tuner hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperband_tuner = kt.Hyperband(\n",
    "    build_model, objective=\"val_accuracy\", seed=33,\n",
    "    max_epochs=10, factor=3, hyperband_iterations=4,\n",
    "    overwrite=True, directory=\"sign_mnist\", project_name=\"hyperband\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponownie pozbywamy sie logow poprzedniego modelu zeby tensorboard mial miejsce z ktorego bedzie mogl pobierac informacje o modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf ./logs/fit/tuner/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunujemy hiperparametry z wczesniej zadeklarowanymi callbackami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 95 Complete [00h 02m 00s]\n",
      "val_accuracy: 0.05714285746216774\n",
      "\n",
      "Best val_accuracy So Far: 1.0\n",
      "Total elapsed time: 00h 23m 39s\n"
     ]
    }
   ],
   "source": [
    "hyperband_tuner.search(x_train, y_train, epochs=20, validation_split=0.2, callbacks=[early_stopping_cb,tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pobieramy najlepsze 2 wyniki z ktorych pobieramy najlepszy model wraz z jego parametrami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parametry hyperband{'num_conv_layers': 2, '0_conv_filters': 256, '0_conv_kernel_size': 3, '0_pooling': 'max', 'num_dense_layers': 2, '0_n_neurons': 64, '0_dropout': 0.0, 'learning_rate': 0.00304325168554751, 'optimizer': 'adam', '1_conv_filters': 256, '1_conv_kernel_size': 3, '1_pooling': 'max', '2_conv_filters': 96, '2_conv_kernel_size': 5, '2_pooling': 'max', '1_n_neurons': 192, '1_dropout': 0.30000000000000004, '2_n_neurons': 448, '2_dropout': 0.0, 'tuner/epochs': 10, 'tuner/initial_epoch': 4, 'tuner/bracket': 2, 'tuner/round': 2, 'tuner/trial_id': '0012'}\n"
     ]
    }
   ],
   "source": [
    "two_best_model_hyperband = hyperband_tuner.get_best_models(num_models=2)\n",
    "best_model_hyperband = two_best_model_hyperband[0]\n",
    "params_hyperband = hyperband_tuner.get_best_hyperparameters(num_trials=5)\n",
    "print(f\"Parametry hyperband{params_hyperband[0].values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby wykorzystac mozliwosci ktore gwarantuje nam TensorBoard nalezy uzyc nastepujacej komendy w konsoli\n",
    "\n",
    "```sh\n",
    "tensorboard --logdir logs/fit/second/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit/tuner/ --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dzieki powyzszej komendzie mozemy zobaczyc jak radzily sobie wszystkie tunowane modele,\n",
    "odrazu widac ktore z nich radzily sobie slabo, ktore przecietnie, i jakie okazaly sie bardzo dobre w tym przypadku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przygotowujemy sobie miejsce do wstawienia informacji o fitowaniu najlepszego modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf ./logs/fit/best/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/best/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitujemy nasz wytunowany model na danych treningowych wraz z callbackami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "180/180 [==============================] - 11s 63ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 2.7938e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "180/180 [==============================] - 11s 61ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 9.0082e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "180/180 [==============================] - 11s 61ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 7.9733e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "180/180 [==============================] - 11s 62ms/step - loss: 0.0631 - accuracy: 0.9805 - val_loss: 0.0505 - val_accuracy: 0.9798\n",
      "Epoch 5/5\n",
      "180/180 [==============================] - 12s 65ms/step - loss: 0.0286 - accuracy: 0.9897 - val_loss: 0.0017 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x319329f90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_hyperband.fit(x_train, y_train, validation_split=0.2, epochs=5,callbacks=[tensorboard_callback,early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ewaluujemy nasz model na danych testowych i wypisujemy dokladnosc i strate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 4s 17ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Model po tunowaniu na danych testowych otrzymuje dokladnosci na poziomie: 1.0.\n",
      "Z strata na poziomie: 0.001710424548946321\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = best_model_hyperband.evaluate(x_test,y_test)\n",
    "print(f\"Model po tunowaniu na danych testowych otrzymuje dokladnosci na poziomie: {test_accuracy}.\\nZ strata na poziomie: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na koniec sprawdzamy jak sprawdzil sie najlepszy model\n",
    "\n",
    "```sh\n",
    "tensorboard --logdir logs/fit/best/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit/best/ --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj widac ze model dziala wybitnie na danych. Dokladnosc niemal odrazu osiaga 100% przy niewielkiej stracie.\n",
    "\n",
    "Widac jak wiele zyskujemy uzywajac tunera. Ktory utworzyl model ktory w 2 epoki osiaga wynik 100% kiedy model skladany recznie po 15 epokach moze sie pochwalic 78% dokladnoscia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jesli proces nie chce sie uruchomic to warto sprawdzic czy poprzedni nie dziala w tle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%lsof` not found.\n"
     ]
    }
   ],
   "source": [
    "%lsof -i :6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
